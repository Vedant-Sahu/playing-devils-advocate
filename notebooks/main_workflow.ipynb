{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f466eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MODEL_NAME': 'gpt-4o', 'TEACHER_MODEL': 'o3', 'GRADING_MODEL': None, 'COORDINATOR_MODEL': 'gpt-4o', 'STUDENT_MODEL': 'gpt-4o', 'CRITIQUE_EVAL_MODEL': 'gpt-4o-mini'}\n"
     ]
    }
   ],
   "source": [
    "# imports and config constants (refactored to per-agent modules)\n",
    "import os, json, pathlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import importlib\n",
    "from IPython.display import Image\n",
    "\n",
    "# Load .env \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd().parent \n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Display model environment variables used by agents\n",
    "print({k: os.getenv(k) for k in [\n",
    "    \"MODEL_NAME\",\n",
    "    \"TEACHER_MODEL\",\n",
    "    \"GRADING_MODEL\",\n",
    "    \"COORDINATOR_MODEL\",\n",
    "    \"STUDENT_MODEL\",\n",
    "    \"CRITIQUE_EVAL_MODEL\",\n",
    "]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e1afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and config constants (refactored to per-agent modules)\n",
    "import os, json, pathlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "from typing_extensions import TypedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd().parent \n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.graphs.adaptive_refinement_graph import create_adaptive_refinement_graph, create_initial_state as adaptive_state\n",
    "from src.graphs.baseline_graph import create_baseline_graph, create_initial_state as baseline_state\n",
    "from src.utils.gpqa_sampler import create_gpqa_quiz\n",
    "\n",
    "THRESHOLD = 0.7\n",
    "MAX_ITERS = 3\n",
    "BASE_OUT = pathlib.Path(\"results\")\n",
    "BASE_OUT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b78859e-0cc4-4a9b-be86-9e4f1a2bdaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline\n",
    "baseline_graph = create_baseline_graph()\n",
    "\n",
    "# Run adaptive\n",
    "adaptive_graph = create_adaptive_refinement_graph()\n",
    "# adaptive_results = adaptive.invoke(adaptive_state(gpqa_quiz=QUIZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5f8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: initialize TruLens OpenAI provider to compute simple relevance\n",
    "try:\n",
    "    from trulens.providers.openai import OpenAI as TruOpenAI\n",
    "    tru_provider = TruOpenAI(model_engine=\"gpt-4o-mini\")\n",
    "except Exception as e:\n",
    "    tru_provider = None\n",
    "    print(\"TruLens provider init failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9fb1dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Initialized with db url sqlite:///edu_refine.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n",
      "âœ… experimental Feature.OTEL_TRACING enabled.\n",
      "ðŸ”’ experimental Feature.OTEL_TRACING is enabled and cannot be changed.\n",
      "instrumenting <class 'langgraph.graph.state.StateGraph'> for base <class 'langgraph.graph.state.StateGraph'>\n",
      "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.graph.state.CompiledStateGraph'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.pregel.main.Pregel'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langgraph.graph.state.StateGraph'> for base <class 'langgraph.graph.state.StateGraph'>\n",
      "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.graph.state.CompiledStateGraph'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.pregel.main.Pregel'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n"
     ]
    }
   ],
   "source": [
    "# TruLens LangGraph recorder (logs interactions)\n",
    "try:\n",
    "    from trulens.core.database.connector.default import DefaultDBConnector\n",
    "    from trulens.core.session import TruSession\n",
    "    from trulens.apps.langgraph import TruGraph\n",
    "\n",
    "    connector = DefaultDBConnector(database_url=\"sqlite:///edu_refine.sqlite\")\n",
    "    tru_session = TruSession(connector=connector)\n",
    "\n",
    "    # Recorder for adaptive refinement graph\n",
    "    tru_recorder = TruGraph(adaptive_graph, app_name=\"Educational Refinement Agent\", app_version=\"mvp-adaptive\")\n",
    "\n",
    "    # Recorder for baseline graph\n",
    "    tru_baseline_recorder = TruGraph(baseline_graph, app_name=\"Educational Refinement Agent\", app_version=\"mvp-baseline\")\n",
    "except Exception as e:\n",
    "    tru_recorder = None\n",
    "    tru_baseline_recorder = None\n",
    "    print(\"TruGraph init failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b181adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "display(Image(adaptive_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1688d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPQA cache from: C:\\Users\\vedan\\Desktop\\playing-devils-advocate\\data\\cache\\gpqa_main_Physics_train.json\n",
      "Loaded 182 Physics questions from gpqa_main\n"
     ]
    }
   ],
   "source": [
    "initial_state = {\n",
    "    \"gpqa_question\": quiz[0],\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"max_iters\": MAX_ITERS,\n",
    "    \"history\": []\n",
    "}\n",
    "\n",
    "if 'tru_recorder' in globals() and tru_recorder is not None:\n",
    "    with tru_recorder as recording:\n",
    "        state = adaptive_graph.invoke(initial_state, config={\"recursion_limit\": 30})\n",
    "else:\n",
    "    state = adaptive_graph.invoke(initial_state, config={\"recursion_limit\": 30})\n",
    "\n",
    "# Build results dict (single-answer evaluation)\n",
    "results = {\n",
    "    \"dataset\": \"GPQA\",\n",
    "    \"subset\": locals().get(\"subset\", \"gpqa_main\"),\n",
    "    \"domain\": locals().get(\"domain\", \"Physics\"),\n",
    "    \"gpqa_index\": locals().get(\"idx\", None),\n",
    "    \"record_id\": state.get(\"gpqa_question\", {}).get(\"id\"),\n",
    "    \"question\": state.get(\"gpqa_question\", {}).get(\"question\", \"\"),\n",
    "    \"iterations\": state.get(\"iteration\", 0),\n",
    "    \"final_scores\": state.get(\"reward_scores\", {}),\n",
    "    \"final_explanation\": state.get(\"explanation\", \"\"),\n",
    "    \"history\": state.get(\"history\", []),\n",
    "    \"gpqa_question\": state.get(\"gpqa_question\", {}),\n",
    "    \"single_answer\": state.get(\"single_answer\", \"\"),\n",
    "    \"single_explanation\": state.get(\"single_explanation\", \"\"),\n",
    "    \"quiz_results\": state.get(\"quiz_results\", {}),\n",
    "}\n",
    "\n",
    "print(\"Dataset:\", results[\"dataset\"], results[\"subset\"], results[\"domain\"]) \n",
    "print(\"Record ID:\", results[\"record_id\"]) \n",
    "print(\"Question:\", results[\"question\"]) \n",
    "print(\"Iterations:\", results[\"iterations\"]) \n",
    "print(\"Final scores:\", results[\"final_scores\"]) \n",
    "print(\"\\nFinal explanation (truncated to 800 chars):\\n\")\n",
    "print(results[\"final_explanation\"][:800])\n",
    "\n",
    "# Print question block\n",
    "qobj = results.get(\"gpqa_question\", {})\n",
    "if qobj:\n",
    "    print(\"\\nGPQA Question:\")\n",
    "    print(f\"- ID: {qobj.get('id','')}\")\n",
    "    print(f\"  Stem: {qobj.get('question','').strip()}\")\n",
    "    for opt in qobj.get(\"options\", []):\n",
    "        print(f\"  {opt}\")\n",
    "    print(f\"  Correct: {qobj.get('correct','?')}\")\n",
    "\n",
    "# Single answer and explanation\n",
    "print(\"\\nSingle Answer Evaluation:\")\n",
    "print(f\"- Predicted: {results.get('single_answer','?')}\")\n",
    "print(f\"- One-sentence explanation: {results.get('single_explanation','')}\")\n",
    "\n",
    "quiz_results = results.get(\"quiz_results\", {})\n",
    "if quiz_results:\n",
    "    print(\"\\nQuiz Results:\")\n",
    "    print(f\"- Correct: {quiz_results.get('is_correct', False)}\")\n",
    "    sim = quiz_results.get(\"explanation_similarity\", 0.0)\n",
    "    print(f\"- Explanation similarity: {sim*100:.1f}%\")\n",
    "\n",
    "# Persist results to results/ with timestamped filename\n",
    "_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def _slugify(s: str, limit: int = 40) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    slug = \"\".join(ch if ch.isalnum() else \"_\" for ch in s)\n",
    "    return slug[:limit] if limit > 0 else slug\n",
    "\n",
    "_id_part = results.get(\"record_id\") or _slugify(results[\"question\"]) or \"run\"\n",
    "outfile = BASE_OUT / f\"run_{_ts}_{_id_part}.json\"\n",
    "record = {\n",
    "    \"timestamp\": _ts,\n",
    "    \"dataset\": results[\"dataset\"],\n",
    "    \"subset\": results[\"subset\"],\n",
    "    \"domain\": results[\"domain\"],\n",
    "    \"gpqa_index\": results.get(\"gpqa_index\"),\n",
    "    \"record_id\": results.get(\"record_id\"),\n",
    "    \"question\": results[\"question\"],\n",
    "    \"final_explanation\": results[\"final_explanation\"],\n",
    "    \"iterations\": results[\"iterations\"],\n",
    "    \"final_scores\": results[\"final_scores\"],\n",
    "    \"gpqa_question\": results.get(\"gpqa_question\", {}),\n",
    "    \"single_answer\": results.get(\"single_answer\", \"\"),\n",
    "    \"single_explanation\": results.get(\"single_explanation\", \"\"),\n",
    "    \"quiz_results\": results.get(\"quiz_results\", {}),\n",
    "}\n",
    "outfile.write_text(json.dumps(record, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"\\nSaved results to: {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40e1866f-93b1-4e25-a0e4-af342a8a485f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking question 1/5...\n",
      "Asking question 2/5...\n",
      "Asking question 3/5...\n",
      "Asking question 4/5...\n",
      "Asking question 5/5...\n"
     ]
    }
   ],
   "source": [
    "baseline_summary = {\n",
    "    \"total_questions\": 0,\n",
    "    \"correct_counts\": {},\n",
    "    \"justification_scores\": {}\n",
    "}\n",
    "\n",
    "adaptive_summary = {\n",
    "    \"total_questions\": 0,\n",
    "    \"correct_counts\": {},\n",
    "    \"justification_scores\": {}\n",
    "}\n",
    "\n",
    "baseline_explanations = []\n",
    "adaptive_explanations = []\n",
    "\n",
    "for i in range(len(quiz)):\n",
    "    \n",
    "    print(f\"Asking question {i+1}/{len(quiz)}...\")\n",
    "    gpqa_question = quiz[i]\n",
    "    \n",
    "    # Run baseline with recording\n",
    "    if 'baseline_recorder' in globals() and baseline_recorder is not None:\n",
    "        with baseline_recorder as recording:\n",
    "            baseline_results = baseline_graph.invoke(baseline_state(gpqa_question),\n",
    "                                                    config={\"recursion_limit\": 30})\n",
    "    baseline_summary = update_summary(baseline_summary, baseline_results['quiz_results'])\n",
    "    entry = {'id': baseline_results['gpqa_question']['id'], \n",
    "             'explanation': baseline_results['explanation']}\n",
    "    baseline_explanations.append(entry)\n",
    "                                     \n",
    "    # Run adaptive with recording  \n",
    "    if 'adaptive_recorder' in globals() and adaptive_recorder is not None:\n",
    "        with adaptive_recorder as recording:\n",
    "            adaptive_results = adaptive_graph.invoke(adaptive_state(gpqa_question, max_iters = 3),\n",
    "                                                    config={\"recursion_limit\": 30})\n",
    "    adaptive_summary = update_summary(adaptive_summary, adaptive_results['quiz_results'])\n",
    "    entry = {'id': adaptive_results['gpqa_question']['id'], \n",
    "             'explanation': adaptive_results['explanation']}\n",
    "    adaptive_explanations.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11006879-a584-43f0-a359-cc424fc28523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>persona</th>\n",
       "      <th>correct_counts</th>\n",
       "      <th>justification_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advanced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>struggling</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>skeptical_misconception</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>practical</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theoretical</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   persona  correct_counts  justification_scores\n",
       "0                 advanced             0.0                   2.6\n",
       "1               struggling             0.0                   2.2\n",
       "2  skeptical_misconception             0.0                   2.2\n",
       "3                practical             0.0                   2.4\n",
       "4              theoretical             0.0                   2.2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_adapt = results_to_df(adaptive_summary)\n",
    "df_adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "036f5dc0-6c3c-49bc-b846-19f54dba1323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>persona</th>\n",
       "      <th>correct_counts</th>\n",
       "      <th>justification_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advanced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>struggling</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>skeptical_misconception</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>practical</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theoretical</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   persona  correct_counts  justification_scores\n",
       "0                 advanced             0.0                   2.0\n",
       "1               struggling             0.0                   1.8\n",
       "2  skeptical_misconception             0.0                   1.8\n",
       "3                practical             0.0                   2.0\n",
       "4              theoretical             0.0                   2.2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base = results_to_df(baseline_summary)\n",
    "df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb640de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee847b970bca4c208c050480d1ca4877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://localhost:51264 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# launch TruLens dashboard to view agent interactions\n",
    "from trulens.dashboard import run_dashboard\n",
    "run_dashboard()  # prints a local URL (e.g., http://localhost:8501); open it in your browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b335402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPQA cache from: C:\\Users\\teono\\Desktop\\CS329T\\playing-devils-advocate\\data\\cache\\gpqa_main_Physics_train.json\n",
      "Loaded 187 Physics questions from gpqa_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e10b9e65a3445fbf9e7497bbc114e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Baseline (Physics):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20 results to results\\baseline_batch_20251115_172542.json\n"
     ]
    }
   ],
   "source": [
    "# Baseline batch run: 20 Physics questions (progress + JSON)\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except Exception:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "subset = \"gpqa_main\"\n",
    "domain = \"Physics\"\n",
    "seed = 100\n",
    "num_questions = 20\n",
    "# Per-question delay (seconds) to avoid rate limits\n",
    "delay_secs = 10\n",
    "\n",
    "quiz, indices = create_gpqa_quiz(subset=subset, domain=domain, seed=seed, num_questions=num_questions)\n",
    "\n",
    "try:\n",
    "    _bg = baseline_graph  # type: ignore[name-defined]\n",
    "except Exception:\n",
    "    _bg = create_baseline_graph()\n",
    "\n",
    "rows = []\n",
    "\n",
    "# If TruLens baseline recorder is available, wrap the loop so each invoke is traced\n",
    "if 'tru_baseline_recorder' in globals() and tru_baseline_recorder is not None:\n",
    "    context_manager = tru_baseline_recorder\n",
    "else:\n",
    "    from contextlib import nullcontext\n",
    "    context_manager = nullcontext()\n",
    "\n",
    "with context_manager as recording:\n",
    "    for q in tqdm(quiz, total=len(quiz), desc=\"Baseline (Physics)\"):\n",
    "        init = baseline_state(gpqa_question=q, threshold=THRESHOLD, max_iters=1)\n",
    "        state = _bg.invoke(init, config={\"recursion_limit\": 10})\n",
    "        gpqa_q = state.get(\"gpqa_question\", {})\n",
    "        quiz_res = state.get(\"quiz_results\", {}) or {}\n",
    "\n",
    "        # correct letter and option text\n",
    "        correct_letter = str(gpqa_q.get(\"correct\", \"\")).strip().upper()\n",
    "        correct_text = \"\"\n",
    "        opts = [str(x) for x in (gpqa_q.get(\"options\", []) or [])]\n",
    "        if correct_letter in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "            for opt in opts:\n",
    "                if len(opt) >= 2 and opt[0].upper() == correct_letter:\n",
    "                    correct_text = opt[2:].strip() if len(opt) > 2 else \"\"\n",
    "                    break\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": gpqa_q.get(\"id\"),\n",
    "            \"question\": gpqa_q.get(\"question\", \"\"),\n",
    "            \"correct\": correct_letter,\n",
    "            \"correct_text\": correct_text,\n",
    "            \"teacher_explanation\": state.get(\"explanation\", \"\"),\n",
    "            \"llm_answer\": state.get(\"single_answer\", \"\"),\n",
    "            \"llm_one_sentence\": state.get(\"single_explanation\", \"\"),\n",
    "            \"similarity\": quiz_res.get(\"explanation_similarity\", 0.0),\n",
    "            \"is_correct\": bool(quiz_res.get(\"is_correct\", False)),\n",
    "        })\n",
    "\n",
    "        # Wait between requests to avoid rate limits\n",
    "        time.sleep(delay_secs)\n",
    "\n",
    "_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "outfile = Path(\"results\") / f\"baseline_batch_{_ts}.json\"\n",
    "outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "outfile.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Saved {len(rows)} results to {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d5bd7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPQA cache from: C:\\Users\\teono\\Desktop\\CS329T\\playing-devils-advocate\\data\\cache\\gpqa_main_Physics_train.json\n",
      "Loaded 187 Physics questions from gpqa_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c54d36e71434a849d37d4bb6de6b533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adaptive (Physics):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20 results to results\\adaptive_batch_20251116_161618.json\n"
     ]
    }
   ],
   "source": [
    "# Adaptive batch run: 20 Physics questions (progress + JSON)\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except Exception:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "subset = \"gpqa_main\"\n",
    "domain = \"Physics\"\n",
    "seed = 100\n",
    "num_questions = 20\n",
    "# Per-question delay (seconds) to avoid rate limits\n",
    "delay_secs = 10\n",
    "\n",
    "quiz, indices = create_gpqa_quiz(subset=subset, domain=domain, seed=seed, num_questions=num_questions)\n",
    "\n",
    "try:\n",
    "    _ag = adaptive_graph  # type: ignore[name-defined]\n",
    "except Exception:\n",
    "    _ag = create_adaptive_refinement_graph()\n",
    "\n",
    "rows = []\n",
    "\n",
    "# If TruLens adaptive recorder is available, wrap the loop so each invoke is traced\n",
    "if 'tru_recorder' in globals() and tru_recorder is not None:\n",
    "    context_manager = tru_recorder\n",
    "else:\n",
    "    from contextlib import nullcontext\n",
    "    context_manager = nullcontext()\n",
    "\n",
    "_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "outfile = Path(\"results\") / f\"adaptive_batch_{_ts}.json\"\n",
    "outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with context_manager as recording:\n",
    "    for q in tqdm(quiz, total=len(quiz), desc=\"Adaptive (Physics)\"):\n",
    "        init = adaptive_state(gpqa_question=q, threshold=THRESHOLD, max_iters=MAX_ITERS)\n",
    "        state = _ag.invoke(init, config={\"recursion_limit\": 30})\n",
    "        gpqa_q = state.get(\"gpqa_question\", {})\n",
    "        quiz_res = state.get(\"quiz_results\", {}) or {}\n",
    "\n",
    "        correct_letter = str(gpqa_q.get(\"correct\", \"\")).strip().upper()\n",
    "        correct_text = \"\"\n",
    "        opts = [str(x) for x in (gpqa_q.get(\"options\", []) or [])]\n",
    "        if correct_letter in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "            for opt in opts:\n",
    "                if len(opt) >= 2 and opt[0].upper() == correct_letter:\n",
    "                    correct_text = opt[2:].strip() if len(opt) > 2 else \"\"\n",
    "                    break\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": gpqa_q.get(\"id\"),\n",
    "            \"question\": gpqa_q.get(\"question\", \"\"),\n",
    "            \"correct\": correct_letter,\n",
    "            \"correct_text\": correct_text,\n",
    "            \"teacher_explanation\": state.get(\"explanation\", \"\"),\n",
    "            \"llm_answer\": state.get(\"single_answer\", \"\"),\n",
    "            \"llm_one_sentence\": state.get(\"single_explanation\", \"\"),\n",
    "            \"similarity\": quiz_res.get(\"explanation_similarity\", 0.0),\n",
    "            \"is_correct\": bool(quiz_res.get(\"is_correct\", False)),\n",
    "        })\n",
    "\n",
    "        outfile.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "        time.sleep(delay_secs)\n",
    "\n",
    "print(f\"Saved {len(rows)} results to {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc9ef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective teacher model: o3\n",
      "Loading GPQA cache from: C:\\Users\\teono\\Desktop\\CS329T\\playing-devils-advocate\\data\\cache\\gpqa_main_Physics_train.json\n",
      "Loaded 187 Physics questions from gpqa_main\n",
      "Question preview: Measuring stellar inclinations is fundamental in both stellar and exoplanetary research. However, it presents a significant challenge. Assuming that stellar inclinations follow an isotropic distributi \n",
      "\n",
      "Teacher explanation length: 0\n",
      "Teacher explanation preview: \n",
      "WARNING: teacher_explain returned empty text.\n",
      "Baseline explanation length: 0\n",
      "Baseline explanation preview: \n"
     ]
    }
   ],
   "source": [
    "from src.config.agent_config import _model_for_role\n",
    "from src.agents.teacher_agent import teacher_explain\n",
    "from src.utils.gpqa_sampler import create_gpqa_quiz\n",
    "\n",
    "subset = \"gpqa_main\"\n",
    "domain = \"Physics\"\n",
    "seed = 123\n",
    "print(\"Effective teacher model:\", _model_for_role(\"teacher\"))\n",
    "\n",
    "try:\n",
    "    quiz, _ = create_gpqa_quiz(subset=subset, domain=domain, seed=seed, num_questions=1)\n",
    "    q = quiz[0][\"question\"]\n",
    "    print(\"Question preview:\", q[:200].replace(\"\\n\",\" \"), \"\\n\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load GPQA question:\", e)\n",
    "    q = \"What is the speed of light in vacuum?\"\n",
    "\n",
    "try:\n",
    "    text = teacher_explain(q, mode=\"adaptive\", student_feedback=None, word_cap=180, max_tokens=500)\n",
    "    print(\"Teacher explanation length:\", len(text))\n",
    "    print(\"Teacher explanation preview:\", text[:400])\n",
    "    if not text.strip():\n",
    "        print(\"WARNING: teacher_explain returned empty text.\")\n",
    "except Exception as e:\n",
    "    print(\"teacher_explain raised:\", repr(e))\n",
    "\n",
    "try:\n",
    "    text2 = teacher_explain(q, mode=\"baseline\", student_feedback=None, word_cap=180, max_tokens=500)\n",
    "    print(\"Baseline explanation length:\", len(text2))\n",
    "    print(\"Baseline explanation preview:\", text2[:200])\n",
    "except Exception as e:\n",
    "    print(\"teacher_explain baseline raised:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91477f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPQA cache from: C:\\Users\\teono\\Desktop\\CS329T\\playing-devils-advocate\\data\\cache\\gpqa_main_Physics_train.json\n",
      "Loaded 187 Physics questions from gpqa_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99df251bdaf483cb78617e61e1fcee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Zero-shot (Physics):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 results to results\\zeroshot_batch_20251116_212104.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except Exception:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from src.config.agent_config import _llm\n",
    "from src.utils.parsing import _extract_json, extract_letter_a_to_d, extract_one_sentence\n",
    "from src.agents.grading_agent import grade_gpqa_single\n",
    "from src.utils.gpqa_sampler import create_gpqa_quiz\n",
    "\n",
    "subset = \"gpqa_main\"\n",
    "domain = \"Physics\"\n",
    "seed = 100\n",
    "num_questions = 100\n",
    "index = 50\n",
    "delay_secs = 10\n",
    "\n",
    "quiz, indices = create_gpqa_quiz(subset=subset, domain=domain, seed=seed, index=index, num_questions=num_questions)\n",
    "\n",
    "\n",
    "def zero_shot_single_answer(gpqa_question):\n",
    "    llm = _llm(temperature=1.0, json_mode=True, role=\"answerer\", max_tokens=500)\n",
    "    sys = SystemMessage(content=(\n",
    "        \"Return ONLY valid JSON with keys 'answer' and 'explanation'. \"\n",
    "        \"Constraints: 'answer' must be exactly one of ['A','B','C','D'] (uppercase). \"\n",
    "        \"'explanation' must be a single sentence.\"\n",
    "    ))\n",
    "\n",
    "    qstem = str(gpqa_question.get(\"question\", \"\"))\n",
    "    options_list = [str(x) for x in (gpqa_question.get(\"options\", []) or [])]\n",
    "    options_text = \"\\n\".join(options_list)\n",
    "\n",
    "    hum = HumanMessage(content=(\n",
    "        \"Question: \" + qstem + \"\\n\"\n",
    "        + \"Options:\\n\" + options_text + \"\\n\"\n",
    "        + \"Respond as JSON only.\"\n",
    "    ))\n",
    "\n",
    "    resp = llm.invoke([sys, hum])\n",
    "    raw_obj = resp.content\n",
    "    raw_text = raw_obj if isinstance(raw_obj, str) else str(raw_obj)\n",
    "\n",
    "    try:\n",
    "        parsed = raw_obj if isinstance(raw_obj, dict) else _extract_json(raw_text)\n",
    "    except Exception:\n",
    "        parsed = {}\n",
    "\n",
    "    letter = str(parsed.get(\"answer\", \"\")).strip().upper()\n",
    "    if letter not in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "        alt = str(parsed.get(\"choice\", parsed.get(\"letter\", \"\"))).strip().upper()\n",
    "        if alt in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "            letter = alt\n",
    "\n",
    "    if letter not in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "        fallback_letter = extract_letter_a_to_d(raw_text) or \"\"\n",
    "        if not fallback_letter:\n",
    "            pairs = []\n",
    "            for opt in options_list:\n",
    "                opt = opt.strip()\n",
    "                if len(opt) >= 3 and opt[1] in \").\" and opt[0].upper() in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "                    label = opt[0].upper()\n",
    "                    body = opt[2:].strip()\n",
    "                    pairs.append((label, body))\n",
    "                elif \")\" in opt:\n",
    "                    idx = opt.find(\")\")\n",
    "                    if idx > 0:\n",
    "                        label = opt[:idx].strip().upper()\n",
    "                        body = opt[idx+1:].strip()\n",
    "                        if label in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "                            pairs.append((label, body))\n",
    "            raw_low = str(raw_text).lower()\n",
    "            hits = [lab for (lab, body) in pairs if body and body.lower() in raw_low]\n",
    "            if len(hits) == 1:\n",
    "                fallback_letter = hits[0]\n",
    "        if fallback_letter in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "            letter = fallback_letter\n",
    "\n",
    "    if letter not in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "        enforce_llm = _llm(temperature=0.0, json_mode=False, role=\"answerer\")\n",
    "        enforce_sys = SystemMessage(content=(\n",
    "            \"Return ONLY a single capital letter among A, B, C, D for the question below. No punctuation or explanation.\"\n",
    "        ))\n",
    "        enforce_hum = HumanMessage(content=(\n",
    "            \"Question: \" + qstem + \"\\n\"\n",
    "            + \"Options:\\n\" + options_text + \"\\n\"\n",
    "        ))\n",
    "        enforce_resp = enforce_llm.invoke([enforce_sys, enforce_hum])\n",
    "        enforce_raw = enforce_resp.content if isinstance(enforce_resp.content, str) else str(enforce_resp.content)\n",
    "        import re\n",
    "        m = re.search(r\"\\b([ABCD])\\b\", enforce_raw.upper())\n",
    "        letter = m.group(1) if m else \"\"\n",
    "\n",
    "    explanation_text = parsed.get(\"explanation\") if isinstance(parsed, dict) else None\n",
    "    one_sentence = str(explanation_text).strip() if isinstance(explanation_text, str) and explanation_text.strip() else extract_one_sentence(raw_text)\n",
    "    return {\"letter\": letter, \"one_sentence\": one_sentence, \"raw\": raw_text}\n",
    "\n",
    "\n",
    "rows = []\n",
    "for q in tqdm(quiz, total=len(quiz), desc=\"Zero-shot (Physics)\"):\n",
    "    res = zero_shot_single_answer(q)\n",
    "\n",
    "    correct_letter = str(q.get(\"correct\", \"\")).strip().upper()\n",
    "    correct_text = \"\"\n",
    "    opts = [str(x) for x in (q.get(\"options\", []) or [])]\n",
    "    if correct_letter in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "        for opt in opts:\n",
    "            if len(opt) >= 2 and opt[0].upper() == correct_letter:\n",
    "                correct_text = opt[2:].strip() if len(opt) > 2 else \"\"\n",
    "                break\n",
    "\n",
    "    graded = grade_gpqa_single(q, res.get(\"letter\", \"\"), res.get(\"one_sentence\", \"\"))\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": q.get(\"id\"),\n",
    "        \"question\": q.get(\"question\", \"\"),\n",
    "        \"correct\": correct_letter,\n",
    "        \"correct_text\": correct_text,\n",
    "        \"zeroshot_answer\": res.get(\"letter\", \"\"),\n",
    "        \"zeroshot_one_sentence\": res.get(\"one_sentence\", \"\"),\n",
    "        \"similarity\": graded.get(\"explanation_similarity\", 0.0),\n",
    "        \"is_correct\": bool(graded.get(\"is_correct\", False)),\n",
    "    })\n",
    "\n",
    "    time.sleep(delay_secs)\n",
    "\n",
    "_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "outfile = Path(\"results\") / f\"zeroshot_batch_{_ts}.json\"\n",
    "outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "outfile.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Saved {len(rows)} results to {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826f343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPQA cache from: C:\\Users\\teono\\Desktop\\CS329T\\playing-devils-advocate\\data\\cache\\gpqa_main_Physics_train.json\n",
      "Loaded 187 Physics questions from gpqa_main\n",
      "Saved 10 results to results\\zeroshot_fixed_ids_20251117_100123.json\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot evaluation on fixed record IDs (5 runs each) with randomized option order per run\n",
    "import json, random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from src.config.agent_config import _llm\n",
    "from src.utils.parsing import _extract_json, extract_letter_a_to_d, extract_one_sentence\n",
    "from src.agents.grading_agent import grade_gpqa_single\n",
    "from src.utils.gpqa_loader import GPQALoader\n",
    "\n",
    "# Target record IDs (order preserved)\n",
    "TARGET_IDS: List[str] = [\n",
    "    \"recLB0EkQ54bYVhnd\",\n",
    "    \"recMicVBcqy1xM1jq\",\n",
    "    \"recNPa63g37t1COiO\",\n",
    "    \"recOj90oKbiyjLcBd\",\n",
    "    \"recQ0eDZk1sxnKVqh\",\n",
    "    \"recSl0g9xnRRB2e0L\",\n",
    "    \"recTns0XOb6qlewR9\",\n",
    "    \"recWSIn23pglAdv4f\",\n",
    "    \"recYE7Rss564wKABO\",\n",
    "    \"recYI852ugl1pFhAv\",\n",
    "]\n",
    "\n",
    "# Build a direct zero-shot single-answer function (no teacher explanation)\n",
    "def zero_shot_single_answer(gpqa_question: Dict) -> Dict[str, str]:\n",
    "    llm = _llm(temperature=1.0, json_mode=True, role=\"answerer\", max_tokens=500)\n",
    "    sys = SystemMessage(content=(\n",
    "        \"Return ONLY valid JSON with keys 'answer' and 'explanation'. \"\n",
    "        \"Constraints: 'answer' must be exactly one of ['A','B','C','D'] (uppercase). \"\n",
    "        \"'explanation' must be a single sentence.\"\n",
    "    ))\n",
    "\n",
    "    qstem = str(gpqa_question.get(\"question\", \"\"))\n",
    "    options_list = [str(x) for x in (gpqa_question.get(\"options\", []) or [])]\n",
    "    options_text = \"\\n\".join(options_list)\n",
    "\n",
    "    hum = HumanMessage(content=(\n",
    "        \"Question: \" + qstem + \"\\n\"\n",
    "        + \"Options:\\n\" + options_text + \"\\n\"\n",
    "        + \"Respond as JSON only.\"\n",
    "    ))\n",
    "\n",
    "    resp = llm.invoke([sys, hum])\n",
    "    raw_obj = resp.content\n",
    "    raw_text = raw_obj if isinstance(raw_obj, str) else str(raw_obj)\n",
    "\n",
    "    try:\n",
    "        parsed = raw_obj if isinstance(raw_obj, dict) else _extract_json(raw_text)\n",
    "    except Exception:\n",
    "        parsed = {}\n",
    "\n",
    "    letter = str(parsed.get(\"answer\", \"\")).strip().upper()\n",
    "    if letter not in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "        alt = str(parsed.get(\"choice\", parsed.get(\"letter\", \"\"))).strip().upper()\n",
    "        if alt in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "            letter = alt\n",
    "\n",
    "    if letter not in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "        fallback_letter = extract_letter_a_to_d(raw_text) or \"\"\n",
    "        if not fallback_letter:\n",
    "            pairs = []\n",
    "            for opt in options_list:\n",
    "                opt = opt.strip()\n",
    "                if len(opt) >= 3 and opt[1] in \").\" and opt[0].upper() in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "                    label = opt[0].upper()\n",
    "                    body = opt[2:].strip()\n",
    "                    pairs.append((label, body))\n",
    "                elif \")\" in opt:\n",
    "                    idx = opt.find(\")\")\n",
    "                    if idx > 0:\n",
    "                        label = opt[:idx].strip().upper()\n",
    "                        body = opt[idx+1:].strip()\n",
    "                        if label in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "                            pairs.append((label, body))\n",
    "            raw_low = str(raw_text).lower()\n",
    "            hits = [lab for (lab, body) in pairs if body and body.lower() in raw_low]\n",
    "            if len(hits) == 1:\n",
    "                fallback_letter = hits[0]\n",
    "        if fallback_letter in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "            letter = fallback_letter\n",
    "\n",
    "    # Final enforcement to ensure a valid A-D answer\n",
    "    if letter not in {\"A\",\"B\",\"C\",\"D\"}:\n",
    "        enforce_llm = _llm(temperature=0.0, json_mode=False, role=\"answerer\")\n",
    "        enforce_sys = SystemMessage(content=(\n",
    "            \"Return ONLY a single capital letter among A, B, C, D for the question below. No punctuation or explanation.\"\n",
    "        ))\n",
    "        enforce_hum = HumanMessage(content=(\n",
    "            \"Question: \" + qstem + \"\\n\"\n",
    "            + \"Options:\\n\" + options_text + \"\\n\"\n",
    "        ))\n",
    "        enforce_resp = enforce_llm.invoke([enforce_sys, enforce_hum])\n",
    "        enforce_raw = enforce_resp.content if isinstance(enforce_resp.content, str) else str(enforce_resp.content)\n",
    "        import re\n",
    "        m = re.search(r\"\\b([ABCD])\\b\", str(enforce_raw).upper())\n",
    "        letter = m.group(1) if m else \"\"\n",
    "\n",
    "    explanation_text = parsed.get(\"explanation\") if isinstance(parsed, dict) else None\n",
    "    one_sentence = str(explanation_text).strip() if isinstance(explanation_text, str) and explanation_text.strip() else extract_one_sentence(raw_text)\n",
    "    return {\"letter\": letter, \"one_sentence\": one_sentence, \"raw\": raw_text}\n",
    "\n",
    "# Load GPQA and select the specified record IDs\n",
    "subset = \"gpqa_main\"\n",
    "domain = \"Physics\"\n",
    "loader = GPQALoader(subset, domain)\n",
    "\n",
    "# Map by both 'id' and 'record_id' for robustness\n",
    "q_by_key: Dict[str, Dict] = {}\n",
    "for q in loader.questions:\n",
    "    _id = str(q.get(\"id\") or \"\").strip()\n",
    "    _rid = str(q.get(\"record_id\") or \"\").strip()\n",
    "    if _id:\n",
    "        q_by_key[_id] = q\n",
    "    if _rid:\n",
    "        q_by_key[_rid] = q\n",
    "\n",
    "selected_ids = [rid for rid in TARGET_IDS if rid in q_by_key]\n",
    "missing = [rid for rid in TARGET_IDS if rid not in q_by_key]\n",
    "if missing:\n",
    "    print(\"Warning: missing record IDs not found in cache:\", missing)\n",
    "\n",
    "# Run zero-shot 5 times per question with randomized option order and write JSON report\n",
    "n_runs = 5\n",
    "results_rows: List[Dict] = []\n",
    "letters = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "for rid in selected_ids:\n",
    "    src = q_by_key[rid]\n",
    "    qstem = str(src.get(\"question\", \"\"))\n",
    "    gold_text = str(src.get(\"correct_answer\", \"\"))\n",
    "    wrongs_full = [str(x) for x in (src.get(\"incorrect_answers\") or [])]\n",
    "    # pad to 3 incorrects\n",
    "    wrongs = (wrongs_full + [\"\"] * 3)[:3]\n",
    "\n",
    "    correct_hits = 0\n",
    "    sims: List[float] = []\n",
    "\n",
    "    for _ in range(n_runs):\n",
    "        # Construct randomized options and correct letter per run\n",
    "        opts_texts = [gold_text] + wrongs[:3]\n",
    "        random.shuffle(opts_texts)\n",
    "        options = [f\"{letters[i]}) {opts_texts[i]}\" for i in range(4)]\n",
    "        correct_letter = letters[opts_texts.index(gold_text)] if gold_text in opts_texts else \"\"\n",
    "\n",
    "        q_run = {\n",
    "            \"id\": rid,\n",
    "            \"question\": qstem,\n",
    "            \"options\": options,\n",
    "            \"correct\": correct_letter,\n",
    "            \"expert_explanation\": str(src.get(\"expert_explanation\", src.get(\"explanation\", \"\"))),\n",
    "        }\n",
    "\n",
    "        res = zero_shot_single_answer(q_run)\n",
    "        graded = grade_gpqa_single(q_run, res.get(\"letter\", \"\"), res.get(\"one_sentence\", \"\"))\n",
    "        correct_hits += 1 if graded.get(\"is_correct\", False) else 0\n",
    "        sims.append(float(graded.get(\"explanation_similarity\", 0.0)))\n",
    "\n",
    "    row = {\n",
    "        \"id\": rid,\n",
    "        \"question\": qstem,\n",
    "        \"correct_answer\": gold_text,\n",
    "        \"llm_correct_percentage\": (100.0 * correct_hits / float(n_runs)) if n_runs > 0 else 0.0,\n",
    "        \"avg_similarity_to_expert\": (sum(sims) / len(sims)) if sims else 0.0,\n",
    "    }\n",
    "    results_rows.append(row)\n",
    "\n",
    "_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "outfile = Path(\"results\") / f\"zeroshot_fixed_ids_{_ts}.json\"\n",
    "outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "outfile.write_text(json.dumps(results_rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Saved {len(results_rows)} results to {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f8536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPQA cache from: C:\\Users\\teono\\Desktop\\CS329T\\playing-devils-advocate\\data\\cache\\gpqa_main_Physics_train.json\n",
      "Loaded 187 Physics questions from gpqa_main\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     80\u001b[39m q_run = {\n\u001b[32m     81\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: rid,\n\u001b[32m     82\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: qstem,\n\u001b[32m   (...)\u001b[39m\u001b[32m     85\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mexpert_explanation\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(src.get(\u001b[33m\"\u001b[39m\u001b[33mexpert_explanation\u001b[39m\u001b[33m\"\u001b[39m, src.get(\u001b[33m\"\u001b[39m\u001b[33mexplanation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))),\n\u001b[32m     86\u001b[39m }\n\u001b[32m     88\u001b[39m init = baseline_state(gpqa_question=q_run)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m state = \u001b[43m_bg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecursion_limit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m quiz_res = state.get(\u001b[33m\"\u001b[39m\u001b[33mquiz_results\u001b[39m\u001b[33m\"\u001b[39m, {}) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(quiz_res.get(\u001b[33m\"\u001b[39m\u001b[33mis_correct\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\core\\otel\\instrument.py:244\u001b[39m, in \u001b[36minstrument.__call__.<locals>.sync_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    243\u001b[39m ret = convert_to_generator(func, instance, args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(ret) == \u001b[33m\"\u001b[39m\u001b[33mis_not_generator\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    245\u001b[39m     res = \u001b[38;5;28mnext\u001b[39m(ret)\n\u001b[32m    246\u001b[39m     \u001b[38;5;66;03m# Check that there are no more entries in the generator.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\core\\otel\\instrument.py:283\u001b[39m, in \u001b[36minstrument.__call__.<locals>.convert_to_generator\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m     func_exception = e\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[43m_finalize_span\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspan_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43monly_set_user_defined_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspan_end_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\core\\otel\\instrument.py:195\u001b[39m, in \u001b[36m_finalize_span\u001b[39m\u001b[34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[39m\n\u001b[32m    193\u001b[39m exception = func_exception \u001b[38;5;129;01mor\u001b[39;00m attributes_exception\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\core\\otel\\instrument.py:266\u001b[39m, in \u001b[36minstrument.__call__.<locals>.convert_to_generator\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# Run function.\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, types.GeneratorType):\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mis_generator\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\langgraph\\pregel\\main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\apps\\langgraph\\tru_graph.py:547\u001b[39m, in \u001b[36mTruGraph._wrap_stream_generator.<locals>.instrumented_generator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minstrumented_generator\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moriginal_generator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Each chunk typically contains node updates\u001b[39;49;00m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\langgraph\\pregel\\main.py:2657\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2655\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2656\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2657\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2662\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2663\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2664\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2666\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2667\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\apps\\langgraph\\tru_graph.py:721\u001b[39m, in \u001b[36mTruGraph._instrument_runnable_callable_method.<locals>.filtered_wrapper\u001b[39m\u001b[34m(wrapped, instance, args, kwargs)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;66;03m# For user-defined nodes, apply full instrumentation\u001b[39;00m\n\u001b[32m    717\u001b[39m instrumented_method = instrument(\n\u001b[32m    718\u001b[39m     span_type=SpanAttributes.SpanType.GRAPH_NODE,\n\u001b[32m    719\u001b[39m     attributes=attributes_func,\n\u001b[32m    720\u001b[39m )(wrapped)\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstrumented_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\core\\otel\\instrument.py:244\u001b[39m, in \u001b[36minstrument.__call__.<locals>.sync_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    243\u001b[39m ret = convert_to_generator(func, instance, args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(ret) == \u001b[33m\"\u001b[39m\u001b[33mis_not_generator\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    245\u001b[39m     res = \u001b[38;5;28mnext\u001b[39m(ret)\n\u001b[32m    246\u001b[39m     \u001b[38;5;66;03m# Check that there are no more entries in the generator.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\core\\otel\\instrument.py:283\u001b[39m, in \u001b[36minstrument.__call__.<locals>.convert_to_generator\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m     func_exception = e\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[43m_finalize_span\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspan_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43monly_set_user_defined_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspan_end_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\core\\otel\\instrument.py:195\u001b[39m, in \u001b[36m_finalize_span\u001b[39m\u001b[34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[39m\n\u001b[32m    193\u001b[39m exception = func_exception \u001b[38;5;129;01mor\u001b[39;00m attributes_exception\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\core\\otel\\instrument.py:266\u001b[39m, in \u001b[36minstrument.__call__.<locals>.convert_to_generator\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# Run function.\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, types.GeneratorType):\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mis_generator\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\Desktop\\CS329T\\playing-devils-advocate\\src\\agents\\grading_agent.py:84\u001b[39m, in \u001b[36mgrading_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Single-answer mode if present, else legacy multi-persona\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msingle_answer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     results = \u001b[43mgrade_gpqa_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgpqa_question\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msingle_answer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msingle_explanation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     90\u001b[39m     student_answers = state.get(\u001b[33m\"\u001b[39m\u001b[33mstudent_answers\u001b[39m\u001b[33m\"\u001b[39m, {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\Desktop\\CS329T\\playing-devils-advocate\\src\\agents\\grading_agent.py:63\u001b[39m, in \u001b[36mgrade_gpqa_single\u001b[39m\u001b[34m(gpqa_question, letter, one_sentence)\u001b[39m\n\u001b[32m     60\u001b[39m is_correct = (letter.strip().upper() == gold_letter) \u001b[38;5;28;01mif\u001b[39;00m gold_letter \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     62\u001b[39m expert = \u001b[38;5;28mstr\u001b[39m(gpqa_question.get(\u001b[33m\"\u001b[39m\u001b[33mexpert_explanation\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m gpqa_question.get(\u001b[33m\"\u001b[39m\u001b[33mexplanation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m jr = \u001b[43mjudge_explanation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_sentence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m score = \u001b[38;5;28mint\u001b[39m(jr[\u001b[33m\"\u001b[39m\u001b[33mexplanation_score\u001b[39m\u001b[33m\"\u001b[39m])  \u001b[38;5;66;03m# 1-5\u001b[39;00m\n\u001b[32m     65\u001b[39m sim = \u001b[38;5;28mmax\u001b[39m(\u001b[32m0.0\u001b[39m, \u001b[38;5;28mmin\u001b[39m(\u001b[32m1.0\u001b[39m, score / \u001b[32m5.0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\core\\otel\\instrument.py:244\u001b[39m, in \u001b[36minstrument.__call__.<locals>.sync_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    243\u001b[39m ret = convert_to_generator(func, instance, args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(ret) == \u001b[33m\"\u001b[39m\u001b[33mis_not_generator\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    245\u001b[39m     res = \u001b[38;5;28mnext\u001b[39m(ret)\n\u001b[32m    246\u001b[39m     \u001b[38;5;66;03m# Check that there are no more entries in the generator.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\core\\otel\\instrument.py:283\u001b[39m, in \u001b[36minstrument.__call__.<locals>.convert_to_generator\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m     func_exception = e\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[43m_finalize_span\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspan_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43monly_set_user_defined_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspan_end_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\core\\otel\\instrument.py:195\u001b[39m, in \u001b[36m_finalize_span\u001b[39m\u001b[34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[39m\n\u001b[32m    193\u001b[39m exception = func_exception \u001b[38;5;129;01mor\u001b[39;00m attributes_exception\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\trulens\\core\\otel\\instrument.py:266\u001b[39m, in \u001b[36minstrument.__call__.<locals>.convert_to_generator\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# Run function.\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, types.GeneratorType):\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mis_generator\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\Desktop\\CS329T\\playing-devils-advocate\\src\\agents\\judge_agent.py:40\u001b[39m, in \u001b[36mjudge_explanation\u001b[39m\u001b[34m(expert_explanation, student_explanation)\u001b[39m\n\u001b[32m     34\u001b[39m hum_content = (\n\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mExpert Explanation:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + expert_explanation.strip() + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m     + \u001b[33m\"\u001b[39m\u001b[33mStudent Explanation:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + student_explanation.strip()\n\u001b[32m     37\u001b[39m )\n\u001b[32m     38\u001b[39m hum = HumanMessage(content=hum_content)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m resp = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhum\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m raw = resp.content\n\u001b[32m     42\u001b[39m parsed = raw \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _extract_json(raw \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(raw))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1025\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     **kwargs: Any,\n\u001b[32m   1023\u001b[39m ) -> LLMResult:\n\u001b[32m   1024\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:842\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    841\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m         )\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    850\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1183\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1182\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1185\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1186\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1187\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1188\u001b[39m ):\n\u001b[32m   1189\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1151\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1148\u001b[39m payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1150\u001b[39m     raw_response = (\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m     )\n\u001b[32m   1155\u001b[39m     response = raw_response.parse()\n\u001b[32m   1156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:183\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    178\u001b[39m         response_format=response_format,\n\u001b[32m    179\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    180\u001b[39m         input_tools=chat_completion_tools,\n\u001b[32m    181\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teono\\anaconda3\\envs\\315env\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "During task with name 'grading' and id 'b7ea63c9-ac3b-d93c-24cc-7449849fa7b6'"
     ]
    }
   ],
   "source": [
    "# Baseline graph evaluation on fixed record IDs (5 runs each) with randomized option order per run\n",
    "import json, random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "from src.utils.gpqa_loader import GPQALoader\n",
    "from src.graphs.baseline_graph import create_baseline_graph, create_initial_state as baseline_state\n",
    "\n",
    "# Use existing compiled graph if available; else create a new one\n",
    "try:\n",
    "    _bg = baseline_graph  # type: ignore[name-defined]\n",
    "except Exception:\n",
    "    _bg = create_baseline_graph()\n",
    "\n",
    "# Target record IDs\n",
    "TARGET_IDS: List[str] = [\n",
    "    \"recLB0EkQ54bYVhnd\",\n",
    "    \"recMicVBcqy1xM1jq\",\n",
    "    \"recNPa63g37t1COiO\",\n",
    "    \"recOj90oKbiyjLcBd\",\n",
    "    \"recQ0eDZk1sxnKVqh\",\n",
    "    \"recSl0g9xnRRB2e0L\",\n",
    "    \"recTns0XOb6qlewR9\",\n",
    "    \"recWSIn23pglAdv4f\",\n",
    "    \"recYE7Rss564wKABO\",\n",
    "    \"recYI852ugl1pFhAv\",\n",
    "]\n",
    "\n",
    "# Load GPQA and map by both 'id' and 'record_id'\n",
    "subset = \"gpqa_main\"\n",
    "domain = \"Physics\"\n",
    "loader = GPQALoader(subset, domain)\n",
    "\n",
    "q_by_key: Dict[str, Dict] = {}\n",
    "for q in loader.questions:\n",
    "    _id = str(q.get(\"id\") or \"\").strip()\n",
    "    _rid = str(q.get(\"record_id\") or \"\").strip()\n",
    "    if _id:\n",
    "        q_by_key[_id] = q\n",
    "    if _rid:\n",
    "        q_by_key[_rid] = q\n",
    "\n",
    "selected_ids = [rid for rid in TARGET_IDS if rid in q_by_key]\n",
    "missing = [rid for rid in TARGET_IDS if rid not in q_by_key]\n",
    "if missing:\n",
    "    print(\"Warning: missing record IDs not found in cache:\", missing)\n",
    "\n",
    "# Run baseline graph 5 times per selected question with randomized option order\n",
    "n_runs = 1\n",
    "rows: List[Dict] = []\n",
    "letters = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# Optional tracing with TruLens if available\n",
    "if 'tru_baseline_recorder' in globals() and tru_baseline_recorder is not None:\n",
    "    context_manager = tru_baseline_recorder\n",
    "else:\n",
    "    from contextlib import nullcontext\n",
    "    context_manager = nullcontext()\n",
    "\n",
    "with context_manager as recording:\n",
    "    for rid in selected_ids:\n",
    "        src = q_by_key[rid]\n",
    "        qstem = str(src.get(\"question\", \"\"))\n",
    "        gold_text = str(src.get(\"correct_answer\", \"\"))\n",
    "        wrongs_full = [str(x) for x in (src.get(\"incorrect_answers\") or [])]\n",
    "        # pad to 3 incorrects\n",
    "        wrongs = (wrongs_full + [\"\"] * 3)[:3]\n",
    "\n",
    "        correct_hits = 0\n",
    "        sims: List[float] = []\n",
    "\n",
    "        for _ in range(n_runs):\n",
    "            # Construct randomized options and correct letter per run\n",
    "            opts_texts = [gold_text] + wrongs[:3]\n",
    "            random.shuffle(opts_texts)\n",
    "            options = [f\"{letters[i]}) {opts_texts[i]}\" for i in range(4)]\n",
    "            correct_letter = letters[opts_texts.index(gold_text)] if gold_text in opts_texts else \"\"\n",
    "\n",
    "            q_run = {\n",
    "                \"id\": rid,\n",
    "                \"question\": qstem,\n",
    "                \"options\": options,\n",
    "                \"correct\": correct_letter,\n",
    "                \"expert_explanation\": str(src.get(\"expert_explanation\", src.get(\"explanation\", \"\"))),\n",
    "            }\n",
    "\n",
    "            init = baseline_state(gpqa_question=q_run)\n",
    "            state = _bg.invoke(init, config={\"recursion_limit\": 20})\n",
    "            quiz_res = state.get(\"quiz_results\", {}) or {}\n",
    "            if bool(quiz_res.get(\"is_correct\", False)):\n",
    "                correct_hits += 1\n",
    "            sims.append(float(quiz_res.get(\"explanation_similarity\", 0.0)))\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": rid,\n",
    "            \"question\": qstem,\n",
    "            \"correct_answer\": gold_text,\n",
    "            \"llm_correct_percentage\": (100.0 * correct_hits / float(n_runs)) if n_runs > 0 else 0.0,\n",
    "            \"avg_similarity_to_expert\": (sum(sims) / len(sims)) if sims else 0.0,\n",
    "        })\n",
    "\n",
    "_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "outfile = Path(\"results\") / f\"baseline_fixed_ids_{_ts}.json\"\n",
    "outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "outfile.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Saved {len(rows)} results to {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf4eed24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPQA cache from: C:\\Users\\teono\\Desktop\\CS329T\\playing-devils-advocate\\data\\cache\\gpqa_main_Physics_train.json\n",
      "Loaded 187 Physics questions from gpqa_main\n",
      "Saved 10 results to results\\adaptive_fixed_ids_20251117_144908.json\n"
     ]
    }
   ],
   "source": [
    "# Adaptive graph evaluation on fixed record IDs (5 runs each) with randomized option order per run\n",
    "import json, random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "from src.utils.gpqa_loader import GPQALoader\n",
    "from src.graphs.adaptive_refinement_graph import (\n",
    "    create_adaptive_refinement_graph,\n",
    "    create_initial_state as adaptive_state,\n",
    ")\n",
    "\n",
    "# Use existing compiled graph if available; else create a new one\n",
    "try:\n",
    "    _ag = adaptive_graph  # type: ignore[name-defined]\n",
    "except Exception:\n",
    "    _ag = create_adaptive_refinement_graph()\n",
    "\n",
    "# Target record IDs\n",
    "TARGET_IDS: List[str] = [\n",
    "    \"recLB0EkQ54bYVhnd\",\n",
    "    \"recMicVBcqy1xM1jq\",\n",
    "    \"recNPa63g37t1COiO\",\n",
    "    \"recOj90oKbiyjLcBd\",\n",
    "    \"recQ0eDZk1sxnKVqh\",\n",
    "    \"recSl0g9xnRRB2e0L\",\n",
    "    \"recTns0XOb6qlewR9\",\n",
    "    \"recWSIn23pglAdv4f\",\n",
    "    \"recYE7Rss564wKABO\",\n",
    "    \"recYI852ugl1pFhAv\",\n",
    "]\n",
    "\n",
    "# Load GPQA and map by both 'id' and 'record_id'\n",
    "subset = \"gpqa_main\"\n",
    "domain = \"Physics\"\n",
    "loader = GPQALoader(subset, domain)\n",
    "\n",
    "q_by_key: Dict[str, Dict] = {}\n",
    "for q in loader.questions:\n",
    "    _id = str(q.get(\"id\") or \"\").strip()\n",
    "    _rid = str(q.get(\"record_id\") or \"\").strip()\n",
    "    if _id:\n",
    "        q_by_key[_id] = q\n",
    "    if _rid:\n",
    "        q_by_key[_rid] = q\n",
    "\n",
    "selected_ids = [rid for rid in TARGET_IDS if rid in q_by_key]\n",
    "missing = [rid for rid in TARGET_IDS if rid not in q_by_key]\n",
    "if missing:\n",
    "    print(\"Warning: missing record IDs not found in cache:\", missing)\n",
    "\n",
    "# Run adaptive graph 5 times per selected question with randomized option order\n",
    "n_runs = 5\n",
    "rows: List[Dict] = []\n",
    "letters = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# Optional tracing with TruLens if available\n",
    "if 'tru_recorder' in globals() and tru_recorder is not None:\n",
    "    context_manager = tru_recorder\n",
    "else:\n",
    "    from contextlib import nullcontext\n",
    "    context_manager = nullcontext()\n",
    "\n",
    "with context_manager as recording:\n",
    "    for rid in selected_ids:\n",
    "        src = q_by_key[rid]\n",
    "        qstem = str(src.get(\"question\", \"\"))\n",
    "        gold_text = str(src.get(\"correct_answer\", \"\"))\n",
    "        wrongs_full = [str(x) for x in (src.get(\"incorrect_answers\") or [])]\n",
    "        # pad to 3 incorrects\n",
    "        wrongs = (wrongs_full + [\"\"] * 3)[:3]\n",
    "\n",
    "        correct_hits = 0\n",
    "        sims: List[float] = []\n",
    "\n",
    "        for _ in range(n_runs):\n",
    "            # Construct randomized options and correct letter per run\n",
    "            opts_texts = [gold_text] + wrongs[:3]\n",
    "            random.shuffle(opts_texts)\n",
    "            options = [f\"{letters[i]}) {opts_texts[i]}\" for i in range(4)]\n",
    "            correct_letter = letters[opts_texts.index(gold_text)] if gold_text in opts_texts else \"\"\n",
    "\n",
    "            q_run = {\n",
    "                \"id\": rid,\n",
    "                \"question\": qstem,\n",
    "                \"options\": options,\n",
    "                \"correct\": correct_letter,\n",
    "                \"expert_explanation\": str(src.get(\"expert_explanation\", src.get(\"explanation\", \"\"))),\n",
    "            }\n",
    "\n",
    "            # Use global THRESHOLD and MAX_ITERS if defined; else fallback defaults\n",
    "            try:\n",
    "                init = adaptive_state(gpqa_question=q_run, threshold=THRESHOLD, max_iters=MAX_ITERS)  # type: ignore[name-defined]\n",
    "            except Exception:\n",
    "                init = adaptive_state(gpqa_question=q_run)\n",
    "\n",
    "            state = _ag.invoke(init, config={\"recursion_limit\": 30})\n",
    "            quiz_res = state.get(\"quiz_results\", {}) or {}\n",
    "            if bool(quiz_res.get(\"is_correct\", False)):\n",
    "                correct_hits += 1\n",
    "            sims.append(float(quiz_res.get(\"explanation_similarity\", 0.0)))\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": rid,\n",
    "            \"question\": qstem,\n",
    "            \"correct_answer\": gold_text,\n",
    "            \"llm_correct_percentage\": (100.0 * correct_hits / float(n_runs)) if n_runs > 0 else 0.0,\n",
    "            \"avg_similarity_to_expert\": (sum(sims) / len(sims)) if sims else 0.0,\n",
    "        })\n",
    "\n",
    "_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "outfile = Path(\"results\") / f\"adaptive_fixed_ids_{_ts}.json\"\n",
    "outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "outfile.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Saved {len(rows)} results to {outfile}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "315env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
